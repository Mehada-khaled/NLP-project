{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e23da7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0e43c",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bfbcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train = pd.read_csv('E:/sub/NLP/project_dataset/train.csv', encoding='iso-8859-1')\n",
    "test = pd.read_csv('E:/sub/NLP/project_dataset/eval.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ad3bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  < start > so i think we can not live if old pe...   \n",
      "1  < start > so i think we can not live if old pe...   \n",
      "2  < start > so i think we can not live if old pe...   \n",
      "3  < start > so i think we can not live if old pe...   \n",
      "4                  < start > for not use car < end >   \n",
      "\n",
      "                                              target  \n",
      "0  So I think we would not be alive if our ancest...  \n",
      "1  So I think we could not live if older people d...  \n",
      "2  So I think we can not live if old people could...  \n",
      "3  So I think we can not live if old people can n...  \n",
      "4                          Not for use with a car .   \n",
      "-----------------------------------------------------------------------------\n",
      "                                               input  \\\n",
      "0  < start > new and new technology has been intr...   \n",
      "1  < start > new and new technology has been intr...   \n",
      "2  < start > new and new technology has been intr...   \n",
      "3  < start > new and new technology has been intr...   \n",
      "4  < start > one possible outcome is that an envi...   \n",
      "\n",
      "                                              target  \n",
      "0    New technology has been introduced to society .  \n",
      "1  New technology has been introduced into the so...  \n",
      "2  Newer and newer technology has been introduced...  \n",
      "3  Newer and newer technology has been introduced...  \n",
      "4  One possible outcome is that an environmentall...  \n"
     ]
    }
   ],
   "source": [
    "# Define function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Remove punctuation\n",
    "    text=\"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    #adding start and end tokens\n",
    "    text = '<start> ' + text + ' <end>'\n",
    "    \n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "     # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "  \n",
    "    # Join the preprocessed sentences\n",
    "    preprocessed_sentences = []\n",
    "    preprocessed_sentences.append(' '.join(words))\n",
    "    \n",
    "    # Join the preprocessed sentences\n",
    "    preprocessed_text = ' '.join(preprocessed_sentences)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the train text and headlines columns\n",
    "train['input'] = train['input'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed train dataset\n",
    "train.to_csv('preprocessed_train.csv', index=False)\n",
    "# Print the preprocessed train dataset\n",
    "print(train.head())\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "# Apply the preprocessing function to the test text and headlines columns\n",
    "test['input'] = test['input'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed test dataset\n",
    "test.to_csv('preprocessed_test.csv', index=False)\n",
    "# Print the preprocessed test dataset\n",
    "print(test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b157efe",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d06cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for train POS tagging\n",
    "def pos_tagging(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Extract the POS tags\n",
    "    pos_tags_only = [tag[1] for tag in pos_tags]\n",
    "    # Join the POS tags into a string\n",
    "    pos_tags_string = \" \".join(pos_tags_only)\n",
    "    return pos_tags_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cd1e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging to the train text and headlines columns\n",
    "train['input_POS'] = train['input'].apply(pos_tagging)\n",
    "\n",
    "# Apply POS tagging to the test text and headlines columns\n",
    "test['input_POS'] = test['input'].apply(pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c9c7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the POS-tagged train dataset\n",
    "train.to_csv('preprocessed_pos.csv', index=False)\n",
    "\n",
    "# Save the POS-tagged test dataset\n",
    "test.to_csv('preprocessed_pos_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b055a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...\n",
      "1    JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...\n",
      "2    JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...\n",
      "3    JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...\n",
      "4                        JJ NN NN IN RB JJ NN JJ NN NN\n",
      "Name: input_POS, dtype: object\n",
      "-----------------------------------------------------------------------------\n",
      "0    JJ NN NNP JJ CC JJ NN VBZ VBN VBN TO DT NN JJ ...\n",
      "1    JJ NN NNP JJ CC JJ NN VBZ VBN VBN TO DT NN JJ ...\n",
      "2    JJ NN NNP JJ CC JJ NN VBZ VBN VBN TO DT NN JJ ...\n",
      "3    JJ NN NNP JJ CC JJ NN VBZ VBN VBN TO DT NN JJ ...\n",
      "4    JJ NN NNP CD JJ NN VBZ IN DT RB JJ NN IN NN NN...\n",
      "Name: input_POS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['input_POS'].head())\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(test['input_POS'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99559da9",
   "metadata": {},
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66827c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "672452c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 2055, 'so': 1986, 'think': 2224, 'we': 2418, 'can': 300, 'not': 1477, 'live': 1274, 'if': 1085, 'old': 1510, 'people': 1589, 'could': 491, 'find': 849, 'siences': 1960, 'and': 107, 'tecnologies': 2171, 'they': 2216, 'did': 591, 'developped': 588, 'end': 713, 'for': 874, 'use': 2360, 'car': 305, 'here': 1040, 'was': 2407, 'no': 1466, 'promise': 1724, 'of': 1498, 'morning': 1419, 'except': 771, 'that': 2195, 'looked': 1288, 'up': 2353, 'through': 2239, 'the': 2197, 'trees': 2299, 'saw': 1889, 'how': 1069, 'low': 1303, 'forest': 878, 'had': 996, 'swung': 2133, 'thus': 2244, 'even': 752, 'today': 2253, 'sex': 1939, 'is': 1172, 'considered': 457, 'as': 144, 'least': 1241, 'important': 1103, 'topic': 2267, 'in': 1111, 'many': 1339, 'parts': 1573, 'india': 1121, 'image': 1089, 'you': 2491, 'salf': 1879, 'are': 130, 'wark': 2404, 'factory': 810, 'just': 1194, 'to': 2252, 'do': 623, 'one': 1518, 'thing': 2222, 'like': 1261, 'pot': 1661, 'taire': 2139, 'on': 1516, 'fire': 851, 'will': 2450, 'destroy': 577, 'becouse': 207, 'dont': 635, 'know': 1206, 'more': 1417, 'than': 2194, 'draw': 642, 'consumers': 465, 'me': 1359, 'purchase': 1743, 'this': 2231, 'great': 973, 'product': 1711, 'with': 2457, 'all': 75, 'these': 2214, 'amazing': 95, 'ingredients': 1135, 'but': 290, 'actually': 25, 'sometimes': 2012, 'make': 1323, 'something': 2009, 'increase': 1115, 'their': 2199, 'sales': 1878, 'want': 2400, 'talk': 2147, 'about': 3, 'nocive': 1468, 'or': 1529, 'bad': 179, 'products': 1713, 'alcohol': 72, 'hair': 997, 'spray': 2050, 'cigarrets': 367, 'example': 768, 'play': 1618, 'football': 873, 'whenever': 2434, 'olders': 1514, 'it': 1177, 'figures': 843, 'diana': 590, 'krall': 1216, 'wearing': 2421, 'rolex': 1859, 'watch': 2412, 'has': 1016, 'text': 2189, 'suggests': 2115, 'reader': 1769, 'wants': 2402, 'belong': 222, 'restricted': 1836, 'club': 387, 'necessarily': 1448, 'wearers': 2420, 'highly': 1047, 'talented': 2146, 'sophisticated': 2017, 'competent': 422, 'artists': 142, 'he': 1020, 'she': 1945, 'should': 1951, 'also': 86, 'have': 1017, 'there': 2210, 'several': 1937, 'reason': 1778, 'work': 2468, 'media': 1367, 'television': 2176, 'magazines': 1313, 'every': 757, 'stuff': 2094, 'made': 1312, 'publicity': 1741, 'affects': 56, 'emotions': 701, 'around': 139, 'world': 2473, 'encouraging': 710, 'them': 2201, 'buy': 292, 'anything': 119, 'since': 1966, 'most': 1420, 'urban': 2356, 'areas': 132, 'occupied': 1494, 'cars': 318, 'facing': 806, 'an': 105, 'irresistible': 1171, 'problem': 1700, 'air': 70, 'noise': 1469, 'pollution': 1643, 'true': 2306, 'preffer': 1676, 'lower': 1304, 'price': 1691, 'when': 2433, 'international': 1155, 'companies': 412, 'already': 85, 'certification': 334, 'begins': 214, 'send': 1926, 'its': 1180, 'market': 1343, 'consume': 462, 'theirs': 2200, 'because': 203, 'difference': 596, 'between': 231, 'prices': 1692, 'probbably': 1699, 'going': 955, 'affect': 55, 'much': 1431, 'big': 234, 'chance': 339, 'prepare': 1677, 'future': 918, 'life': 1257, 'person': 1600, 'good': 960, 'health': 1022, 'means': 1363, 'things': 2223, 'lost': 1297, 'studies': 2090, 'over': 1545, 'last': 1224, 'years': 2485, 'shown': 1954, 'who': 2442, 'fluoride': 864, 'drinking': 645, 'water': 2414, 'considerably': 456, 'less': 1247, 'cavities': 328, 'nonfluoridated': 1472, 'likewise': 1265, 'theory': 2207, 'generating': 935, 'from': 901, 'concept': 437, 'memory': 1377, 'appropriate': 127, 'way': 2415, 'due': 653, 'limited': 1268, 'spent': 2044, 'time': 2245, 'unmeaningful': 2343, 'subjects': 2101, 'very': 2381, 'fast': 830, 'my': 1435, 'best': 228, 'seconds': 1913, 'hundred': 1074, 'meters': 1388, 'country': 493, 'young': 2493, 'spend': 2041, 'ther': 2208, 'lifestile': 1259, 'society': 1990, 'study': 2092, 'never': 1454, 'opportunity': 1526, 'ca': 295, 'understand': 2332, 'any': 116, 'ideas': 1081, 'situation': 1970, 'interestion': 1153, 'prefer': 1674, 'material': 1351, 'serious': 1933, 'almost': 80, 'doens': 627, 'students': 2088, 'focus': 867, 'only': 1520, 'few': 837, 'intwerested': 1161, 'become': 205, 'experts': 789, 'those': 2234, 'other': 1536, 'waythismakes': 2417, 'need': 1450, 'pay': 1584, 'extra': 798, 'money': 1410, 'repair': 1811, 'roads': 1853, 'what': 2429, 'reduce': 1790, 'amount': 102, 'gas': 923, 'high': 1046, 'tax': 2157, 'income': 1113, 'thought': 2236, 'rise': 1848, 'unfair': 2337, 'mba': 1358, 'background': 178, 'engineering': 718, 'demand': 558, 'case': 320, 'studying': 2093, 'hard': 1008, 'possible': 1656, 'get': 940, 'better': 230, 'grades': 968, 'jobs': 1187, 'school': 1896, 'try': 2310, 'learning': 1238, 'bit': 240, 'science': 1897, 'exist': 775, 'skill': 1975, 'specializing': 2035, 'particular': 1571, 'subject': 2099, 'does': 628, 'suit': 2116, 'our': 1539, 'era': 741, 'which': 2438, 'characterized': 346, 'by': 294, 'diversity': 621, 'innovation': 1138, 'explain': 790, 'points': 1630, 'views': 2389, 'following': 870, 'paragraphs': 1563, 'thinks': 2226, 'differently': 598, 'succeded': 2102, 'activities': 21, 'community': 410, 'place': 1613, 'inculcate': 1120, 'values': 2373, 'members': 1373, 'beacuse': 198, 'broad': 272, 'knowledge': 1208, 'help': 1034, 'major': 1321, 'university': 2341, 'well': 2425, 'correct': 486, 'choice': 361, 'easy': 663, 'why': 2445, 'always': 92, 'brang': 259, 'marks': 1345, 'at': 154, 'home': 1058, 'then': 2203, 'went': 2426, 'olympiad': 1515, 'diverse': 619, 'knowledges': 1210, 'subects': 2098, 'new': 1455, 'her': 1039, 'learn': 1236, 'lots': 1299, 'scientifc': 1899, 'result': 1837, 'imporeved': 1100, 'concordance': 444, 'above': 4, 'agree': 66, 'whole': 2443, 'heartedly': 1028, 'twenty': 2317, 'be': 197, 'fewer': 838, 'third': 2228, 'butt': 291, 'hte': 1071, 'front': 902, 'door': 636, 'store': 2074, 'poeple': 1625, 'see': 1917, 'however': 1070, 'soon': 2016, 'run': 1866, 'out': 1541, 'current': 517, 'rate': 1763, 'utilisation': 2369, 'lecture': 1243, 'professor': 1715, 'oppose': 1527, 'reading': 1770, 'statement': 2060, 'create': 501, 'opportunities': 1525, 'throughts': 2243, 'lives': 1275, 'needs': 1452, 'familys': 819, 'inorder': 1140, 'would': 2478, 'additionally': 33, 'skills': 1977, 'either': 691, 'theoretical': 2204, 'practical': 1666, 'understanding': 2333, 'application': 123, 'chosen': 366, 'give': 945, 'beautiful': 199, 'sell': 1925, 'secondly': 1912, 'film': 846, 'must': 1434, 'clear': 378, 'chir': 360, 'relax': 1803, 'etc': 750, 'put': 1747, 'maind': 1317, 'non': 1470, 'stop': 2072, 'proof': 1728, 'speeding': 2040, 'offense': 1500, 'polices': 1631, 'easily': 661, 'catch': 323, 'compuer': 431, 'first': 854, 'choosed': 363, 'truth': 2309, 'stopped': 2073, 'myself': 1436, 'real': 1772, 'possibility': 1655, 'fucture': 903, 'his': 1052, 'own': 1548, 'field': 840, 'word': 2466, 'benefits': 226, 'information': 1133, 'editors': 675, 'conseder': 452, 'idea': 1078, 'totally': 2269, 'autonomous': 167, 'within': 2458, 'individual': 1123, 'process': 1705, 'defining': 551, 'reality': 1774, 'state': 2058, 'useless': 2364, 'being': 218, 'based': 190, 'experience': 782, 'everything': 761, 'universe': 2340, 'god': 954, 'death': 535, 'some': 2004, 'concepts': 438, 'everyone': 760, 'encourage': 709, 'lot': 1298, 'facts': 811, 'found': 888, 'yet': 2488, 'expensive': 781, 'besides': 227, 'allday': 76, 'living': 1278, 'profits': 1718, 'year': 2484, 'after': 59, 'yaer': 2483, 'numbers': 1485, 'number': 1484, 'decrease': 543, 'took': 2262, 'upon': 2354, 'himself': 1049, 'narrow': 1442, 'research': 1822, 'mystery': 1437, 'others': 1537, 'uncerfied': 2326, 'wood': 2465, 'self': 1923, 'confidence': 447, 'specialized': 2034, 'docters': 625, 'docter': 624, 'done': 633, 'operation': 1522, 'often': 1506, 'before': 210, 'really': 1775, 'job': 1186, 'situstion': 1972, 'traveled': 2293, 'group': 982, 'without': 2459, 'tour': 2271, 'guide': 990, 'teachers': 2163, 'take': 2141, 'youg': 2492, 'genaration': 929, 'test': 2185, 'student': 2087, 'baly': 185, 'otherwise': 1538, 'us': 2359, 'certain': 333, 'participate': 1569, 'kind': 1202, 'meet': 1370, 'second': 1911, 'treasure': 2296, 'locations': 1283, 'implausibly': 1099, 'large': 1223, 'mainly': 1318, 'concetrate': 441, 'questions': 1752, 'asked': 147, 'him': 1048, 'interview': 1157, 'becaese': 200, 'easier': 660, 'economy': 671, 'reflect': 1794, 'costs': 489, 'consumer': 464, 'willingness': 2452, 'set': 1936, 'maximum': 1355, 'security': 1916, 'difficult': 599, 'wonderful': 2463, 'view': 2387, 'throught': 2242, 'bus': 285, 'fashional': 828, 'culture': 514, 'been': 209, 'changed': 342, 'unstable': 2348, 'trip': 2303, 'obvious': 1493, 'returning': 1841, 'tired': 2251, 'night': 1462, 'meant': 1364, 'sleep': 1980, 'nowadays': 1482, 'capable': 303, 'sorts': 2020, 'works': 2472, 'finally': 847, 'fish': 856, 'farming': 825, 'relates': 1801, 'long': 1286, 'term': 2183, 'wastefulness': 2410, 'whcih': 2431, 'pollute': 1640, 'sea': 1908, 'establishing': 748, 'connections': 450, 'researchers': 1823, 'open': 1521, 'projects': 1723, 'now': 1481, 'outstanding': 1543, 'scores': 1904, 'knows': 1214, 'return': 1840, 'furthermore': 916, 'friends': 899, 'nineteen': 1463, 'drive': 646, 'another': 112, 'cities': 368, 'were': 2427, 'smallers': 1983, 'violence': 2390, 'traffic': 2278, 'bothering': 251, 'hazzle': 1019, 'though': 2235, 'advertisement': 49, 'unreal': 2345, 'looks': 1290, 'pretty': 1686, 'opposing': 1528, 'observed': 1489, 'passage': 1576, 'pick': 1610, 'kids': 1200, 'go': 951, 'shopping': 1948, 'early': 657, 'industry': 1127, 'warker': 2405, 'worked': 2469, 'together': 2258, 'kept': 1197, 'anasazi': 106, 'american': 97, 'southwest': 2025, 'temparture': 2179, 'caused': 326, 'droughts': 650, 'support': 2121, 'success': 2105, 'next': 1459, 'while': 2439, 'might': 1391, 'specialize': 2033, 'still': 2067, 'taking': 2144, 'courses': 497, 'both': 250, 'men': 1378, 'dealed': 532, 'situations': 1971, 'unconventional': 2327, 'manner': 1337, 'results': 1839, 'shall': 1941, 'effect': 679, 'exams': 770, 'moreover': 1418, 'risks': 1850, 'teaches': 2164, 'accept': 8, 'winning': 2455, 'losing': 1296, 'lesson': 1249, 'order': 1531, 'achieve': 14, 'objective': 1488, 'initiate': 1136, 'whatever': 2430, 'pointed': 1629, 'mistakes': 1402, 'normal': 1475, 'propose': 1731, 'client': 379, 'construct': 460, 'different': 597, 'legal': 1246, 'transforming': 2285, 'present': 1680, 'days': 528, 'actual': 24, 'fact': 807, 'against': 62, 'looking': 1289, 'moment': 1409, 'taught': 2156, 'able': 1, 'perform': 1594, 'tests': 2187, 'man': 1328, 'whose': 2444, 'hobbies': 1056, 'playing': 1620, 'ball': 182, 'day': 527, 'until': 2350, 'again': 61, 'consulting': 461, 'although': 90, 'lucky': 1306, 'ones': 1519, 'struck': 2084, 'gold': 956, 'simply': 1965, 'vanished': 2374, 'left': 1245, 'homes': 1059, 'forgotten': 881, 'developed': 584, 'obsessions': 1490, 'western': 2428, 'mirage': 1400, 'citizens': 369, 'countries': 492, 'possess': 1651, 'themselves': 2202, 'grip': 978, 'ample': 103, 'exopsure': 777, 'greatly': 975, 'hand': 1000, 'convinced': 484, 'interesting': 1152, 'advertisements': 50, 'everywhere': 762, 'becuase': 208, 'times': 2247, 'face': 802, 'problems': 1702, 'bought': 253, 'back': 177, 'responsible': 1833, 'helpful': 1036, 'conclusion': 443, 'change': 341, 'your': 2496, 'ypu': 2499, 'younger': 2494, 'matter': 1354, 'whether': 2437, 'likes': 1264, 'ever': 756, 'increasing': 1118, 'competancy': 419, 'rates': 1764, 'force': 875, 'into': 1158, 'frequent': 894, 'business': 287, 'model': 1405, 'changes': 343, 'compatible': 417, 'trasisional': 2290, 'flexibility': 861, 'casts': 322, 'douts': 639, 'speaker': 2029, 'mention': 1383, 'gives': 947, 'placebo': 1614, 'illusion': 1087, 'drug': 651, 'effected': 680, 'travel': 2292, 'close': 384, 'suddenly': 2110, 'cost': 487, 'half': 998, 'lifecycle': 1258, 'same': 1881, 'later': 1226, 'according': 12, 'two': 2319, 'technical': 2165, 'begin': 212, 'share': 1943, 'ask': 146, 'weaker': 2419, 'cup': 515, 'form': 882, 'germany': 939, 'conch': 442, 'wan': 2399, 'na': 1438, 'scientists': 1901, 'obtained': 1492, 'investigations': 1166, 'table': 2136, 'began': 211, 'theorise': 2206, 'holistic': 1057, 'understend': 2334, 'thet': 2215, 'dhe': 589, 'trust': 2308, 'theories': 2205, 'migrating': 1392, 'methods': 1390, 'environmental': 735, 'issue': 1175, 'push': 1746, 'consider': 455, 'commute': 411, 'instead': 1145, 'driving': 649, 'continue': 469, 'doing': 630, 'hope': 1061, 'noticed': 1480, 'video': 2384, 'convenient': 480, 'concern': 439, 'using': 2367, 'texetbooks': 2188, 'ability': 0, 'tend': 2182, 'restrict': 1835, 'hours': 1066, 'parking': 1567, 'fees': 834, 'instance': 1144, 'evolution': 764, 'exists': 776, 'depending': 569, 'oin': 1509, 'choose': 362, 'right': 1845, 'am': 94, 'sorry': 2018, 'room': 1861, 'computer': 433, 'opinion': 1523, 'dependend': 567, 'killed': 1201, 'global': 950, 'warming': 2406, 'green': 977, 'house': 1067, 'effects': 685, 'started': 2056, 'recently': 1784, 'examples': 769, 'wrong': 2482, 'ways': 2416, 'got': 962, 'respond': 1830, 'thife': 2221, 'robber': 1854, 'stolen': 2070, 'criminal': 505, 'speed': 2039, 'anderstand': 108, 'dengures': 561, 'beteer': 229, 'axcedant': 172, 'therefore': 2212, 'peaple': 1585, 'automobile': 166, 'sosiety': 2021, 'nature': 1444, 'talkkative': 2148, 'explains': 791, 'project': 1722, 'errounuosly': 744, 'used': 2361, 'three': 2237, 'persons': 1604, 'hoped': 1062, 'lessen': 1248, 'during': 654, 'busy': 289, 'hour': 1065, 'cheaper': 348, 'marco': 1341, 'polo': 1645, 'persian': 1599, 'langage': 1220, 'chinese': 359, 'mongolian': 1412, 'animals': 110, 'elaboration': 692, 'diferents': 594, 'foods': 872, 'lke': 1279, 'oil': 1508, 'brougth': 278, 'imidiatly': 1092, 'weatherrealted': 2422, 'factors': 809, 'such': 2109, 'land': 1219, 'shape': 1942, 'area': 131, 'bay': 196, 'livestock': 1276, 'poultry': 1664, 'once': 1517, 'policy': 1632, 'unwritten': 2352, 'law': 1229, 'furious': 914, 'youth': 2497, 'mental': 1382, 'pressure': 1685, 'creates': 502, 'break': 261, 'cause': 325, 'birds': 237, 'type': 2320, 'internal': 1154, 'compass': 416, 'crystals': 512, 'mineral': 1396, 'magetite': 1314, 'embedded': 699, 'teacher': 2162, 'teach': 2160, 'searching': 1910, 'internet': 1156, 'books': 249, 'stimulated': 2069, 'imagination': 1090, 'quality': 1749, 'book': 247, 'copper': 485, 'scroll': 1907, 'notes': 1478, 'merely': 1386, 'hidden': 1045, 'avivinity': 168, 'cretain': 504, 'river': 1852, 'fields': 841, 'cheapest': 349, 'says': 1891, 'provide': 1738, 'enough': 726, 'fishes': 857, 'futhermore': 917, 'safety': 1874, 'don': 632, 'ts': 2312, 'each': 655, 'academic': 6, 'specialists': 2032, 'progress': 1720, 'worse': 2475, 'fans': 821, 'usually': 2368, 'un': 2323, 'known': 1213, 'includes': 1112, 'risk': 1849, 'inmany': 1137, 'news': 1457, 'newspaper': 1458, 'tv': 2316, 'mang': 1334, 'knowlegable': 1211, 'donnot': 634, 'enjoy': 720, 'posssbale': 1658, 'arguement': 133, 'ages': 64, 'sir': 1968, 'disease': 613, 'save': 1887, 'advertisemnets': 52, 'successful': 2106, 'politicians': 1635, 'tried': 2301, 'somthing': 2014, 'smarter': 1985, 'groundless': 981, 'youngsters': 2495, 'contribute': 475, 'communities': 409, 'extent': 797, 'writing': 2479, 'essay': 747, 'therfore': 2213, 'reduced': 1791, 'sulfur': 2118, 'nitrogen': 1465, 'dioxide': 603, 'hear': 1025, 'distribute': 618, 'point': 1628, 'fair': 814, 'suffering': 2112, 'stupid': 2096, 'maybe': 1357, 'beginning': 213, 'timer': 2246, 'inviroment': 1168, 'food': 871, 'older': 1513, 'remembering': 1810, 'scraping': 1905, 'causes': 327, 'heat': 1029, 'energy': 716, 'huge': 1072, 'difficulties': 600, 'generation': 936, 'adaptable': 29, 'adapt': 28, 'changing': 344, 'technology': 2169, 'ready': 1771, 'education': 677, 'specific': 2037, 'details': 578, 'furethermore': 913, 'denies': 562, 'states': 2061, 'exsit': 795, 'fiction': 839, 'chose': 365, 'italian': 1178, 'solve': 2002, 'solution': 2001, 'personally': 1603, 'diet': 593, 'manage': 1329, 'condition': 445, 'carefully': 314, 'plays': 1621, 'role': 1858, 'aquired': 129, 'rather': 1765, 'care': 311, 'environment': 734, 'consuming': 466, 'ozone': 1554, 'layer': 1232, 'addition': 32, 'nagatice': 1439, 'influence': 1131, 'agriculture': 69, 'responsibilities': 1831, 'said': 1876, 'environments': 737, 'doubt': 638, 'assertion': 150, 'prevents': 1689, 'dental': 564, 'responsibility': 1832, 'iede': 1084, 'thie': 2219, 'critica': 507, 'arguments': 134, 'videos': 2385, 'delivered': 557, 'format': 883, 'familiar': 817, 'reasonable': 1779, 'course': 496, 'local': 1281, 'businesses': 288, 'depended': 566, 'park': 1566, 'visitors': 2394, 'suffered': 2111, 'heart': 1027, 'decease': 537, 'patients': 1583, 'shows': 1955, 'improvement': 1110, 'uaually': 2322, 'fasion': 829, 'ipod': 1170, 'mp': 1429, 'main': 1316, 'part': 1568, 'peole': 1586, 'happily': 1006, 'music': 1433, 'ear': 656, 'movies': 1426, 'producing': 1710, 'vehicles': 2379, 'fuel': 904, 'efficint': 688, 'damage': 522, 'measures': 1366, 'taken': 2142, 'check': 350, 'population': 1648, 'recomended': 1787, 'doctor': 626, 'pier': 1611, 'scientist': 1900, 'discover': 610, 'invent': 1162, 'convenience': 479, 'monthe': 1413, 'skin': 1978, 'actully': 26, 'adevrtising': 37, 'attention': 159, 'newer': 1456, 'fly': 865, 'sky': 1979, 'annoy': 111, 'trafic': 2279, 'jams': 1183, 'offers': 1503, 'possibilities': 1654, 'stay': 2064, 'places': 1615, 'avoid': 169, 'possetion': 1652, 'dream': 643, 'generate': 933, 'natural': 1443, 'processes': 1706, 'human': 1073, 'released': 1805, 'pollutants': 1639, 'nothing': 1479, 'menhadens': 1381, 'primary': 1694, 'source': 2023, 'protein': 1735, 'livestocks': 1277, 'funny': 912, 'especially': 746, 'movie': 1425, 'allows': 78, 'fun': 909, 'buildings': 280, 'costly': 488, 'maintain': 1319, 'child': 354, 'spends': 2043, 'five': 859, 'parents': 1564, 'probably': 1698, 'classmates': 375, 'personality': 1602, 'determine': 580, 'react': 1767, 'towards': 2273, 'broader': 275, 'spectrum': 2038, 'develop': 582, 'competences': 421, 'throughout': 2241, 'someone': 2007, 'talent': 2145, 'ballet': 184, 'believe': 220, 'may': 1356, 'reasons': 1780, 'thinking': 2225, 'power': 1665, 'managing': 1332, 'strategy': 2076, 'making': 1327, 'goals': 953, 'perfect': 1592, 'politik': 1637, 'bismarck': 238, 'manipulated': 1336, 'letter': 1252, 'written': 2480, 'king': 1203, 'sent': 1930, 'french': 893, 'emperor': 702, 'napoleon': 1441, 'iii': 1086, 'complete': 425, 'compared': 414, 'spending': 2042, 'decides': 540, 'explore': 792, 'herself': 1042, 'wanted': 2401, 'say': 1890, 'enjoying': 722, 'putted': 1748, 'definition': 552, 'somebody': 2005, 'matches': 1349, 'owner': 1549, 'posses': 1650, 'dealer': 533, 'shop': 1947, 'offert': 1504, 'conditions': 446, 'obbvious': 1487, 'sold': 1996, 'chain': 336, 'becomes': 206, 'stronger': 2083, 'rabbits': 1754, 'phone': 1607, 'funciton': 910, 'imper': 1098, 'wich': 2446, 'master': 1348, 'call': 296, 'related': 1800, 'carrier': 317, 'importand': 1102, 'solid': 1998, 'base': 189, 'yes': 2487, 'idealism': 1079, 'comunities': 435, 'indonesia': 1125, 'friend': 897, 'eat': 664, 'compensated': 418, 'professional': 1714, 'sorting': 2019, 'issues': 1176, 'solving': 2003, 'risky': 1851, 'prescribed': 1679, 'burning': 284, 'forests': 879, 'grow': 984, 'plentiful': 1623, 'farmers': 824, 'enneoyed': 724, 'sounds': 2022, 'general': 931, 'lies': 1256, 'techology': 2170, 'grown': 985, 'tries': 2302, 'invention': 1163, 'applied': 124, 'hence': 1038, 'children': 356, 'hesitancy': 1043, 'interaction': 1149, 'solidifies': 1999, 'played': 1619, 'latter': 1228, 'career': 313, 'won': 2462, 'championships': 338, 'produce': 1707, 'sofisticated': 1992, 'comfort': 399, 'physical': 1609, 'learned': 1237, 'experinces': 788, 'contradicts': 472, 'influences': 1132, 'economics': 670, 'purpose': 1745, 'owning': 1551, 'release': 1804, 'mind': 1395, 'bass': 193, 'predetor': 1673, 'menhaden': 1379, 'basses': 194, 'curative': 516, 'measure': 1365, 'importance': 1101, 'preventive': 1688, 'medicine': 1369, 'healthy': 1024, 'individuals': 1124, 'attracted': 161, 'wherever': 2436, 'aware': 171, 'disadvantages': 605, 'getting': 941, 'raspsquo': 1762, 'among': 101, 'pleasure': 1622, 'delay': 554, 'disguise': 615, 'infact': 1129, 'incrismt': 1119, 'government': 965, 'public': 1740, 'safty': 1875, 'sphinx': 2045, 'kingdom': 1204, 'stated': 2059, 'interest': 1150, 'math': 1353, 'problemes': 1701, 'score': 1903, 'exam': 766, 'passion': 1580, 'games': 921, 'curricular': 518, 'compulsory': 432, 'attend': 158, 'name': 1440, 'family': 818, 'kyoto': 1217, 'city': 370, 'japan': 1184, 'valuable': 2371, 'temples': 2180, 'remein': 1807, 'board': 242, 'possiable': 1653, 'acqurie': 16, 'ad': 27, 'method': 1389, 'feel': 833, 'happy': 1007, 'exciting': 772, 'memorable': 1374, 'clue': 388, 'rocks': 1857, 'near': 1445, 'coast': 389, 'explorers': 793, 'trying': 2311, 'bring': 269, 'respect': 1829, 'rule': 1864, 'safe': 1872, 'father': 831, 'mother': 1421, 'visit': 2392, 'lecturer': 1244, 'authour': 165, 'insistments': 1143, 'told': 2260, 'working': 2471, 'fluent': 863, 'famous': 820, 'thomas': 2233, 'edison': 674, 'question': 1751, 'loan': 1280, 'bank': 186, 'experiments': 786, 'failing': 812, 'behave': 216, 'spontaneously': 2047, 'praising': 1670, 'noring': 1474, 'conversation': 482, 'surprize': 2128, 'happened': 1002, 'wait': 2397, 'suggest': 2113, 'susidise': 2131, 'poor': 1646, 'resource': 1827, 'improve': 1108, 'company': 413, 'basis': 192, 'aspects': 149, 'helping': 1037, 'makes': 1325, 'postive': 1660, 'tooth': 2265, 'pastes': 1582, 'teeth': 2173, 'briliant': 267, 'brighter': 266, 'ourselves': 1540, 'seems': 1919, 'insufficient': 1146, 'closet': 385, 'space': 2026, 'inefficient': 1128, 'heating': 1030, 'decaying': 536, 'plumbing': 1624, 'harder': 1009, 'houses': 1068, 'advertisemnet': 51, 'thhe': 2218, 'soap': 1987, 'wil': 2448, 'soft': 1993, 'fifteen': 842, 'princess': 1695, 'jennife': 1185, 'lopez': 1292, 'hirring': 1051, 'chimps': 357, 'remember': 1808, 'reply': 1813, 'conceder': 436, 'colecting': 392, 'mass': 1347, 'transport': 2288, 'systems': 2135, 'function': 911, 'primarily': 1693, 'conventional': 481, 'sources': 2024, 'suited': 2117, 'mordern': 1416, 'residents': 1826, 'imagine': 1091, 'fantastic': 822, 'sensation': 1928, 'victory': 2383, 'bakery': 181, 'claims': 372, 'dinosaurs': 602, 'bone': 244, 'structure': 2085, 'similar': 1963, 'endothermy': 715, 'assume': 152, 'ten': 2181, 'ago': 65, 'consists': 459, 'owns': 1552, 'satisfies': 1885, 'transportation': 2289, 'discovery': 611, 'channel': 345, 'wild': 2449, 'requires': 1819, 'braking': 257, 'perspectives': 1605, 'private': 1697, 'sad': 1871, 'firming': 853, 'uses': 2366, 'special': 2030, 'meal': 1360, 'lawyers': 1231, 'extending': 796, 'tell': 2177, 'worst': 2476, 'period': 1596, 'amitabh': 100, 'bacchan': 175, 'offered': 1502, 'enter': 729, 'politics': 1636, 'advantage': 45, 'paid': 1557, 'heavily': 1031, 'financial': 848, 'mr': 1430, 'rejected': 1799, 'offer': 1501, 'knew': 1205, 'polititians': 1638, 'imotions': 1096, 'common': 406, 'desport': 575, 'ground': 980, 'soil': 1995, 'oceans': 1497, 'palce': 1560, 'plant': 1617, 'fires': 852, 'realist': 1773, 'successfull': 2107, 'history': 1053, 'war': 2403, 'development': 587, 'cultural': 513, 'moviments': 1427, 'resulted': 1838, 'helped': 1035, 'occuring': 1495, 'exemple': 773, 'commercials': 404, 'alternative': 88, 'passege': 1578, 'emphesize': 703, 'priscibing': 1696, 'giving': 948, 'turm': 2313, 'disking': 616, 'dry': 652, 'dead': 530, 'shrub': 1956, 'stimulate': 2068, 'growth': 986, 'saved': 1888, 'invest': 1165, 'activity': 22, 'passes': 1579, 'succesfull': 2104, 'lawyer': 1230, 'contracts': 470, 'words': 2467, 'verity': 2380, 'towns': 2276, 'encurge': 711, 'interested': 1151, 'particles': 1570, 'sustain': 2132, 'active': 20, 'harsh': 1015, 'cold': 390, 'guides': 992, 'limit': 1267, 'context': 468, 'contain': 467, 'reliable': 1806, 'dependent': 568, 'agreed': 67, 'dis': 604, 'juju': 1192, 'island': 1173, 'variety': 2375, 'answers': 115, 'subjective': 2100, 'tast': 2155, 'studing': 2091, 'let': 1250, 'electric': 695, 'gasoline': 924, 'effort': 689, 'startet': 2057, 'actions': 19, 'ecocertify': 667, 'user': 2365, 'toothpaste': 2266, 'definelty': 549, 'contruction': 478, 'commercial': 403, 'ford': 876, 'subdued': 2097, 'everybody': 758, 'loves': 1302, 'gone': 959, 'hardship': 1010, 'obtain': 1491, 'experince': 787, 'olde': 1512, 'signs': 1962, 'encounter': 708, 'drivers': 648, 'memorize': 1376, 'where': 2435, 'camera': 299, 'located': 1282, 'toys': 2277, 'advertisments': 53, 'target': 2152, 'false': 815, 'answer': 113, 'sixty': 1973, 'fore': 877, 'sathsh': 1882, 'trasportation': 2291, 'inter': 1147, 'tanmant': 2151, 'bisnes': 239, 'requirement': 1818, 'army': 138, 'sacrifise': 1870, 'preciouse': 1671, 'modify': 1407, 'carbohydarte': 306, 'gelatinized': 928, 'nongelatinized': 1473, 'carbohydrate': 307, 'incorporated': 1114, 'feed': 832, 'utilize': 2370, 'previous': 1690, 'chances': 340, 'appreciated': 126, 'turned': 2314, 'reaults': 1781, 'experiences': 784, 'brings': 270, 'hero': 1041, 'succes': 2103, 'negative': 1453, 'side': 1958, 'suggestion': 2114, 'situaction': 1969, 'reach': 1766, 'components': 429, 'held': 1033, 'map': 1340, 'marked': 1342, 'magnitides': 1315, 'mars': 1346, 'evidence': 763, 'france': 891, 'italy': 1179, 'alone': 82, 'met': 1387, 'nice': 1461, 'producer': 1708, 'song': 2015, 'taiwan': 2140, 'finding': 850, 'petrol': 1606, 'digging': 601, 'procedure': 1704, 'quantity': 1750, 'animal': 109, 'recovered': 1788, 'provides': 1739, 'industries': 1126, 'luce': 1305, 'inwall': 1169, 'earth': 659, 'writting': 2481, 'nearly': 1447, 'unable': 2325, 'show': 1953, 'felings': 835, 'action': 18, 'salt': 1880, 'body': 243, 'trueth': 2307, 'compitition': 424, 'maket': 1326, 'faceing': 804, 'choosing': 364, 'carree': 316, 'software': 1994, 'single': 1967, 'loner': 1285, 'language': 1221, 'deed': 546, 'refuced': 1797, 'unfortunately': 2338, 'isolated': 1174, 'perpose': 1597, 'hardwork': 1011, 'sides': 1959, 'happening': 1003, 'marketing': 1344, 'accounting': 13, 'leadership': 1235, 'communication': 408, 'confirms': 448, 'supports': 2122, 'guess': 989, 'anyone': 118, 'medias': 1368, 'personal': 1601, 'useful': 2362, 'tool': 2263, 'rightly': 1846, 'victims': 2382, 'greedy': 976, 'loosing': 1291, 'gon': 958, 'th': 2192, 'untill': 2351, 'woek': 2460, 'sure': 2123, 'kid': 1199, 'teached': 2161, 'tools': 2264, 'telephones': 2175, 'look': 1287, 'entertain': 730, 'athletics': 155, 'arts': 143, 'arisky': 136, 'strainge': 2075, 'belive': 221, 'outweigh': 1544, 'focuse': 868, 'arkward': 137, 'comes': 398, 'ears': 658, 'event': 753, 'stagnant': 2053, 'woud': 2477, 'traveling': 2295, 'prepation': 1678, 'filled': 845, 'joy': 1189, 'carebendiocside': 312, 'harm': 1012, 'full': 907, 'listen': 1270, 'attract': 160, 'buying': 293, 'fashionable': 827, 'clothes': 386, 'studied': 2089, 'depth': 571, 'topics': 2268, 'understood': 2335, 'caught': 324, 'turns': 2315, 'ashes': 145, 'pottesium': 1663, 'alot': 84, 'acadmic': 7, 'seperatley': 1931, 'management': 1331, 'plan': 1616, 'hire': 1050, 'trained': 2281, 'classes': 374, 'social': 1988, 'oldage': 1511, 'leave': 1242, 'burdens': 283, 'forward': 887, 'althogh': 89, 'treasures': 2297, 'far': 823, 'concerned': 440, 'carusage': 319, 'united': 2339, 'aswell': 153, 'latinamerican': 1227, 'thirdworld': 2229, 'cut': 520, 'thier': 2220, 'workers': 2470, 'willing': 2451, 'sollution': 2000, 'developing': 585, 'efficient': 687, 'america': 96, 'manifacturers': 1335, 'attractive': 162, 'consequently': 454, 'regard': 1798, 'effecting': 681, 'down': 640, 'wife': 2447, 'tokyo': 2259, 'december': 538, 'thereby': 2211, 'running': 1867, 'fuels': 905, 'augumenting': 164, 'decline': 541, 'effection': 682, 'grobal': 979, 'ho': 1055, 'chi': 353, 'minh': 1397, 'develope': 583, 'peolple': 1587, 'types': 2321, 'models': 1406, 'thinmg': 2227, 'greatest': 974, 'gift': 942, 'anyways': 120, 'alon': 81, 'produces': 1709, 'having': 1018, 'necessary': 1449, 'disagree': 607, 'raising': 1759, 'effective': 683, 'oher': 1507, 'mater': 1350, 'discuss': 612, 'dengerous': 559, 'driver': 647, 'trobull': 2304, 'crash': 499, 'streets': 2078, 'allready': 79, 'fixed': 860, 'data': 524, 'nobody': 1467, 'creat': 500, 'litreture': 1272, 'indicates': 1122, 'carful': 315, 'train': 2280, 'pollutions': 1644, 'carbones': 309, 'aboue': 2, 'advance': 42, 'age': 63, 'barrier': 188, 'somethings': 2010, 'simple': 1964, 'appeal': 122, 'nursing': 1486, 'college': 393, 'four': 889, 'mentioned': 1384, 'shops': 1949, 'exactly': 765, 'given': 946, 'adds': 34, 'define': 548, 'critics': 509, 'poins': 1627, 'yellowstone': 2486, 'scorched': 1902, 'species': 2036, 'past': 1581, 'completely': 426, 'baed': 180, 'apply': 125, 'rare': 1761, 'understad': 2331, 'de': 529, 'figurout': 844, 'todays': 2254, 'unreplenishable': 2346, 'thirst': 2230, 'outside': 1542, 'firstly': 855, 'equal': 738, 'rerationship': 1821, 'spoil': 2046, 'needed': 1451, 'alive': 74, 'tims': 2249, 'rich': 1843, 'intrested': 1159, 'politica': 1633, 'sports': 2048, 'technologies': 2168, 'spaise': 2027, 'britain': 271, 'increases': 1117, 'lead': 1234, 'density': 563, 'fractures': 890, 'elderly': 694, 'forget': 880, 'happen': 1001, 'properly': 1729, 'behind': 217, 'completing': 427, 'gradually': 969, 'improved': 1109, 'becase': 202, 'cours': 495, 'gante': 922, 'prove': 1736, 'prefere': 1675, 'attempting': 157, 'scienses': 1898, 'equalifications': 739, 'extraordinary': 799, 'creation': 503, 'gouverment': 963, 'importnat': 1104, 'idease': 1082, 'gave': 927, 'carbon': 308, 'emission': 700, 'combine': 396, 'mean': 1361, 'divided': 622, 'dates': 526, 'raher': 1756, 'come': 397, 'inventions': 1164, 'progressed': 1721, 'technological': 2167, 'promote': 1726, 'system': 2134, 'liberty': 1254, 'pace': 1555, 'deliberate': 555, 'presentable': 1681, 'modals': 1404, 'babies': 173, 'faces': 805, 'bungee': 282, 'jumping': 1193, 'clips': 383, 'juicy': 1190, 'hamburger': 999, 'flying': 866, 'screen': 1906, 'surely': 2124, 'satisfying': 1886, 'eating': 665, 'viewers': 2388, 'sense': 1929, 'hungry': 1075, 'urge': 2357, 'hop': 1060, 'restaurant': 1834, 'nearby': 1446, 'seen': 1920, 'happens': 1005, 'expect': 778, 'accident': 11, 'else': 698, 'makers': 1324, 'proffeser': 1716, 'agrees': 68, 'cheep': 351, 'rais': 1757, 'enviroment': 733, 'healths': 1023, 'peples': 1590, 'maner': 1333, 'secure': 1915, 'comfortable': 400, 'tarvel': 2153, 'destination': 576, 'desire': 573, 'critic': 506, 'costumer': 490, 'lose': 1294, 'critical': 508, 'reflex': 1796, 'guard': 988, 'short': 1950, 'contradicted': 471, 'materials': 1352, 'preson': 1684, 'somethnig': 2011, 'thay': 2196, 'coluld': 395, 'glance': 949, 'accessing': 10, 'mobile': 1403, 'decreased': 544, 'employer': 705, 'commensence': 402, 'art': 140, 'failure': 813, 'strugles': 2086, 'transmit': 2286, 'hebitats': 1032, 'eastern': 662, 'europe': 751, 'considering': 458, 'weekend': 2423, 'exutic': 801, 'luxery': 1307, 'encorragies': 707, 'varying': 2376, 'comforts': 401, 'undeniable': 2328, 'diversifed': 620, 'immediate': 1093, 'seem': 1918, 'move': 1423, 'somewher': 2013, 'twice': 2318, 'performing': 1595, 'routine': 1863, 'actors': 23, 'soldiers': 1997, 'adequately': 36, 'runs': 1868, 'electricity': 696, 'harnessed': 1014, 'polluting': 1642, 'comparing': 415, 'sections': 1914, 'head': 1021, 'phones': 1608, 'seriously': 1934, 'task': 2154, 'clean': 377, 'rolling': 1860, 'stone': 2071, 'gather': 926, 'proverb': 1737, 'educations': 678, 'reqired': 1817, 'testing': 2186, 'declined': 542, 'valueable': 2372, 'english': 719, 'buses': 286, 'reserved': 1825, 'spots': 2049, 'tramway': 2284, 'routes': 1862, 'little': 1273, 'cross': 511, 'street': 2077, 'red': 1789, 'light': 1260, 'dangerous': 523, 'production': 1712, 'communicate': 407, 'ordered': 1532, 'promised': 1725, 'shud': 1957, 'repreated': 1816, 'differance': 595, 'encylopedia': 712, 'viruses': 2391, 'hackers': 995, 'informations': 1134, 'win': 2453, 'court': 498, 'goods': 961, 'hey': 1044, 'likr': 1266, 'machine': 1310, 'controlled': 477, 'comsumers': 434, 'likely': 1263, 'pair': 1558, 'trousers': 2305, 'claim': 371, 'capacity': 304, 'german': 938, 'skilled': 1976, 'endangered': 714, 'breaking': 262, 'selfconfdence': 1924, 'bundle': 281, 'meaningless': 1362, 'disaster': 608, 'adding': 31, 'advertissment': 54, 'unplanly': 2344, 'profit': 1717, 'socialization': 1989, 'toefl': 2255, 'generations': 937, 'educated': 676, 'pass': 1575, 'wisdom': 2456, 'service': 1935, 'volunteerism': 2396, 'arises': 135, 'absence': 5, 'adequate': 35, 'resources': 1828, 'fulfilling': 906, 'consumed': 463, 'drink': 644, 'came': 298, 'impression': 1107, 'drag': 641, 'liked': 1262, 'inside': 1142, 'none': 1471, 'jack': 1181, 'benefit': 225, 'enable': 706, 'worldwide': 2474, 'compete': 420, 'manufactures': 1338, 'pal': 1559, 'composed': 430, 'senior': 1927, 'added': 30, 'advatage': 47, 'level': 1253, 'became': 201, 'member': 1372, 'estimate': 749, 'governments': 966, 'promoting': 1727, 'attributers': 163, 'economic': 669, 'succussful': 2108, 'brave': 260, 'sientist': 1961, 'predators': 1672, 'beneficial': 224, 'control': 476, 'tof': 2256, 'smart': 1984, 'girl': 943, 'chemistry': 352, 'class': 373, 'oxygen': 1553, 'blanket': 241, 'cutting': 521, 'mach': 1309, 'togather': 2257, 'useing': 2363, 'behabiar': 215, 'mentions': 1385, 'mongol': 1411, 'recods': 1786, 'harmful': 1013, 'pollutes': 1641, 'theacher': 2198, 'deaply': 534, 'investing': 1167, 'airplane': 71, 'ship': 1946, 'former': 884, 'belgrano': 219, 'experienced': 783, 'faced': 803, 'oral': 1530, 'examinations': 767, 'critism': 510, 'whitch': 2440, 'walk': 2398, 'town': 2275, 'ng': 1460, 'diseases': 614, 'couple': 494, 'wasteful': 2409, 'idean': 1080, 'read': 1768, 'page': 1556, 'impressed': 1106, 'generally': 932, 'certified': 335, 'supervising': 2120, 'entities': 731, 'expected': 779, 'customers': 519, 'learnng': 1239, 'whlie': 2441, 'gratuation': 972, 'bood': 246, 'bookmaker': 248, 'office': 1505, 'lottery': 1300, 'languages': 1222, 'along': 83, 'lie': 1255, 'detector': 579, 'brain': 255, 'scanning': 1892, 'telling': 2178, 'keeping': 1196, 'ensures': 727, 'date': 525, 'fully': 908, 'happenings': 1004, 'puls': 1742, 'brand': 258, 'wasent': 2408, 'tha': 2193, 'under': 2329, 'voluntad': 2395, 'economyc': 672, 'teenager': 2172, 'experiencing': 785, 'sexual': 1940, 'contrast': 473, 'gender': 930, 'staying': 2065, 'avoiding': 170, 'purer': 1744, 'extreme': 800, 'cases': 321, 'gain': 919, 'weight': 2424, 'immergent': 1095, 'events': 754, 'someday': 2006, 'environmentally': 736, 'friendly': 898, 'attachment': 156, 'keep': 1195, 'riding': 1844, 'effel': 686, 'tower': 2274, 'paris': 1565, 'statue': 2062, 'york': 2490, 'too': 2261, 'alerady': 73, 'heard': 1026, 'adjust': 39, 'learns': 1240, 'complex': 428, 'employee': 704, 'totaly': 2270, 'impact': 1097, 'baby': 174, 'enjoyed': 721, 'lastly': 1225, 'owners': 1550, 'factories': 808, 'overrall': 1547, 'expenses': 780, 'reducing': 1792, 'reaserch': 1777, 'forums': 886, 'broadly': 276, 'majors': 1322, 'stablely': 2051, 'grade': 967, 'exponentially': 794, 'paradise': 1562, 'takes': 2143, 'wonderfull': 2464, 'gained': 920, 'fortune': 885, 'somethig': 2008, 'stand': 2054, 'nowhere': 1483, 'anybody': 117, 'surface': 2125, 'moon': 1414, 'safely': 1873, 'millionaire': 1394, 'managed': 1330, 'childish': 355, 'freshman': 895, 'meiji': 1371, 'classmetes': 376, 'china': 358, 'dislike': 617, 'groups': 983, 'follow': 869, 'strict': 2080, 'schedules': 1895, 'rules': 1865, 'bar': 187, 'specialist': 2031, 'ofcourse': 1499, 'disadvanteges': 606, 'ceace': 329, 'comming': 405, 'popular': 1647, 'taxation': 2158, 'soda': 1991, 'adiction': 38, 'series': 1932, 'guided': 991, 'schedule': 1894, 'accepting': 9, 'opinions': 1524, 'convince': 483, 'advertised': 48, 'poinetd': 1626, 'climate': 380, 'erosion': 743, 'love': 1301, 'actally': 17, 'speak': 2028, 'unsuspection': 2349, 'cannto': 301, 'entruy': 732, 'vees': 2378, 'tampered': 2150, 'prevented': 1687, 'unknown': 2342, 'replaced': 1812, 'innovations': 1139, 'certaily': 332, 'thoes': 2232, 'contrasting': 474, 'impossible': 1105, 'robot': 1855, 'eventually': 755, 'therapist': 2209, 'treatment': 2298, 'approprite': 128, 'whe': 2432, 'aspect': 148, 'conscious': 451, 'consequences': 453, 'laying': 1233, 'watching': 2413, 'programs': 1719, 'mac': 1308, 'size': 1974, 'throughing': 2240, 'design': 572, 'building': 279, 'altough': 91, 'depend': 565, 'protected': 1733, 'called': 297, 'ballast': 183, 'fashion': 826, 'knowing': 1207, 'deal': 531, 'probles': 1703, 'associates': 151, 'competitive': 423, 'race': 1755, 'mislead': 1401, 'habilities': 993, 'politically': 1634, 'engaged': 717, 'generates': 934, 'cell': 331, 'brillant': 268, 'small': 1982, 'boys': 254, 'girls': 944, 'salary': 1877, 'modren': 1408, 'brifly': 265, 'sharing': 1944, 'vecation': 2377, 'advantege': 46, 'youths': 2498, 'acquire': 15, 'immense': 1094, 'satisfaction': 1883, 'ecocertification': 666, 'introduced': 1160, 'ecologically': 668, 'practices': 1668, 'broadcaster': 273, 'proposed': 1732, 'menhadenhresting': 1380, 'reduction': 1793, 'huthis': 1077, 'propleme': 1730, 'sevirl': 1938, 'status': 2063, 'advancement': 43, 'admission': 41, 'breaks': 263, 'bicycles': 233, 'relatively': 1802, 'cheap': 347, 'minimal': 1398, 'maintenance': 1320, 'alsow': 87, 'bi': 232, 'amricans': 104, 'mor': 1415, 'americanc': 98, 'listenig': 1271, 'woman': 2461, 'ecplani': 673, 'nir': 1464, 'dose': 637, 'reflest': 1795, 'decreasing': 545, 'everyday': 759, 'reviewing': 1842, 'stage': 2052, 'graduate': 970, 'guaranteed': 987, 'bachelors': 176, 'degree': 553, 'challenges': 337, 'perserverant': 1598, 'determined': 581, 'strive': 2082, 'minute': 1399, 'despite': 574, 'mad': 1311, 'goverments': 964, 'timescedules': 2248, 'trains': 2282, 'trams': 2283, 'survey': 2130, 'requre': 1820, 'selective': 1922, 'sees': 1921, 'flue': 862, 'surgery': 2127, 'transplant': 2287, 'organs': 1534, 'climbing': 382, 'mountain': 1422, 'cant': 302, 'quite': 1753, 'exertion': 774, 'taxes': 2159, 'further': 915, 'pink': 1612, 'hot': 1063, 'dog': 629, 'familes': 816, 'joined': 1188, 'textiles': 2191, 'key': 1198, 'element': 697, 'esp': 745, 'domain': 631, 'korea': 1215, 'summer': 2119, 'knowlegde': 1212, 'shouldnt': 1952, 'robots': 1856, 'peopie': 1588, 'deeply': 547, 'celebrate': 330, 'pary': 1574, 'upset': 2355, 'depressed': 570, 'reserched': 1824, 'reported': 1815, 'habitant': 994, 'rural': 1869, 'ereas': 742, 'possitive': 1657, 'postion': 1659, 'increased': 1116, 'recent': 1783, 'presented': 1682, 'practitioners': 1669, 'basic': 191, 'orthopaedic': 1535, 'surgeon': 2126, 'bones': 245, 'cardiologist': 310, 'ent': 728, 'nose': 1476, 'throat': 2238, 'definication': 550, 'yong': 2489, 'apparent': 121, 'connected': 449, 'hunter': 1076, 'dengres': 560, 'bird': 236, 'effectively': 684, 'occurs': 1496, 'becayse': 204, 'wasting': 2411, 'satisfied': 1884, 'efforts': 690, 'overcome': 1546, 'rememberd': 1809, 'artist': 141, 'fell': 836, 'afraid': 58, 'terrible': 2184, 'searched': 1909, 'answered': 114, 'undertaking': 2336, 'percent': 1591, 'partients': 1572, 'idiea': 1083, 'delicious': 556, 'golden': 957, 'fries': 900, 'pale': 1561, 'potato': 1662, 'stumps': 2095, 'fried': 896, 'perfection': 1593, 'decide': 539, 'raise': 1758, 'insects': 1141, 'loses': 1295, 'grandparents': 971, 'afther': 60, 'im': 1088, 'brother': 277, 'alwys': 93, 'talks': 2149, 'teh': 2174, 'riks': 1847, 'taht': 2137, 'tahy': 2138, 'los': 1293, 'theyselves': 2217, 'report': 1814, 'vietnam': 2386, 'americans': 99, 'died': 592, 'battlefield': 195, 'bill': 235, 'gates': 925, 'president': 1683, 'window': 2454, 'undergroud': 2330, 'inflaction': 1130, 'tremendous': 2300, 'practiced': 1667, 'hitting': 1054, 'receiving': 1782, 'goal': 952, 'techniques': 2166, 'free': 892, 'lets': 1251, 'jam': 1182, 'stressfull': 2079, 'disatvantages': 609, 'textbooks': 2190, 'eldelery': 693, 'travelers': 2294, 'slow': 1981, 'movements': 1424, 'unability': 2324, 'broadcasting': 274, 'breaus': 264, 'surrounding': 2129, 'admire': 40, 'multi': 1432, 'knowledgement': 1209, 'allow': 77, 'developly': 586, 'climbers': 381, 'coldness': 391, 'lack': 1218, 'brainstorm': 256, 'fishing': 858, 'limits': 1269, 'advancing': 44, 'visiting': 2393, 'ordinary': 1533, 'milk': 1393, 'step': 2066, 'afford': 57, 'randomly': 1760, 'hotel': 1064, 'unsafe': 2347, 'moving': 1428, 'protecting': 1734, 'urged': 2358, 'passed': 1577, 'interact': 1148, 'belongs': 223, 'equipped': 740, 'logically': 1284, 'bottele': 252, 'juise': 1191, 'colourful': 394, 'tourists': 2272, 'tips': 2250, 'striped': 2081, 'memories': 1375, 'enogh': 725, 'posibilty': 1649, 'enjoyment': 723, 'sceene': 1893, 'reckoned': 1785, 'reaonable': 1776}\n",
      "['ability' 'able' 'aboue' ... 'youth' 'youths' 'ypu']\n"
     ]
    }
   ],
   "source": [
    "X_count = cv_count.fit(train['input'])\n",
    "# automatically remove any one character word like a\n",
    "print(X_count.vocabulary_)\n",
    "# unique words in corpus\n",
    "print(cv_count.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c6222db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 2500)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "      ability  able  aboue  about  above  absence  academic  acadmic  accept  \\\n",
      "0           0     0      0      0      0        0         0        0       0   \n",
      "1           0     0      0      0      0        0         0        0       0   \n",
      "2           0     0      0      0      0        0         0        0       0   \n",
      "3           0     0      0      0      0        0         0        0       0   \n",
      "4           0     0      0      0      0        0         0        0       0   \n",
      "...       ...   ...    ...    ...    ...      ...       ...      ...     ...   \n",
      "3011        0     0      0      0      0        0         0        0       0   \n",
      "3012        0     0      0      0      0        0         0        0       0   \n",
      "3013        0     0      0      0      0        0         0        0       0   \n",
      "3014        0     0      0      0      0        0         0        0       0   \n",
      "3015        0     0      0      0      0        0         0        0       0   \n",
      "\n",
      "      accepting  ...  york  you  youg  young  younger  youngsters  your  \\\n",
      "0             0  ...     0    0     0      0        0           0     0   \n",
      "1             0  ...     0    0     0      0        0           0     0   \n",
      "2             0  ...     0    0     0      0        0           0     0   \n",
      "3             0  ...     0    0     0      0        0           0     0   \n",
      "4             0  ...     0    0     0      0        0           0     0   \n",
      "...         ...  ...   ...  ...   ...    ...      ...         ...   ...   \n",
      "3011          0  ...     0    0     0      0        0           0     0   \n",
      "3012          0  ...     0    0     0      1        0           0     0   \n",
      "3013          0  ...     0    0     0      1        0           0     0   \n",
      "3014          0  ...     0    0     0      1        0           0     0   \n",
      "3015          0  ...     0    0     0      1        0           0     0   \n",
      "\n",
      "      youth  youths  ypu  \n",
      "0         0       0    0  \n",
      "1         0       0    0  \n",
      "2         0       0    0  \n",
      "3         0       0    0  \n",
      "4         0       0    0  \n",
      "...     ...     ...  ...  \n",
      "3011      0       0    0  \n",
      "3012      0       0    0  \n",
      "3013      0       0    0  \n",
      "3014      0       0    0  \n",
      "3015      0       0    0  \n",
      "\n",
      "[3016 rows x 2500 columns]\n"
     ]
    }
   ],
   "source": [
    "# create a document term matrix\n",
    "X_count = cv_count.transform(train['input'])\n",
    "print(X_count.shape)\n",
    "#document term matrix\n",
    "print(X_count.toarray())\n",
    "df = pd.DataFrame(X_count.toarray(),columns=cv_count.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a3fc1",
   "metadata": {},
   "source": [
    "# N-Grams Vecorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce1e9a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 9124)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability for</th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>aboue advance</th>\n",
       "      <th>about adding</th>\n",
       "      <th>about alternative</th>\n",
       "      <th>about any</th>\n",
       "      <th>about commensence</th>\n",
       "      <th>about community</th>\n",
       "      <th>about contracts</th>\n",
       "      <th>...</th>\n",
       "      <th>your teeth</th>\n",
       "      <th>your time</th>\n",
       "      <th>your toys</th>\n",
       "      <th>your views</th>\n",
       "      <th>your way</th>\n",
       "      <th>your work</th>\n",
       "      <th>youth end</th>\n",
       "      <th>youth have</th>\n",
       "      <th>youths who</th>\n",
       "      <th>ypu fact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3016 rows  9124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability for  ability to  able to  aboue advance  about adding  \\\n",
       "0               0           0        0              0             0   \n",
       "1               0           0        0              0             0   \n",
       "2               0           0        0              0             0   \n",
       "3               0           0        0              0             0   \n",
       "4               0           0        0              0             0   \n",
       "...           ...         ...      ...            ...           ...   \n",
       "3011            0           0        0              0             0   \n",
       "3012            0           0        0              0             0   \n",
       "3013            0           0        0              0             0   \n",
       "3014            0           0        0              0             0   \n",
       "3015            0           0        0              0             0   \n",
       "\n",
       "      about alternative  about any  about commensence  about community  \\\n",
       "0                     0          0                  0                0   \n",
       "1                     0          0                  0                0   \n",
       "2                     0          0                  0                0   \n",
       "3                     0          0                  0                0   \n",
       "4                     0          0                  0                0   \n",
       "...                 ...        ...                ...              ...   \n",
       "3011                  0          0                  0                0   \n",
       "3012                  0          0                  0                0   \n",
       "3013                  0          0                  0                0   \n",
       "3014                  0          0                  0                0   \n",
       "3015                  0          0                  0                0   \n",
       "\n",
       "      about contracts  ...  your teeth  your time  your toys  your views  \\\n",
       "0                   0  ...           0          0          0           0   \n",
       "1                   0  ...           0          0          0           0   \n",
       "2                   0  ...           0          0          0           0   \n",
       "3                   0  ...           0          0          0           0   \n",
       "4                   0  ...           0          0          0           0   \n",
       "...               ...  ...         ...        ...        ...         ...   \n",
       "3011                0  ...           0          0          0           0   \n",
       "3012                0  ...           0          0          0           0   \n",
       "3013                0  ...           0          0          0           0   \n",
       "3014                0  ...           0          0          0           0   \n",
       "3015                0  ...           0          0          0           0   \n",
       "\n",
       "      your way  your work  youth end  youth have  youths who  ypu fact  \n",
       "0            0          0          0           0           0         0  \n",
       "1            0          0          0           0           0         0  \n",
       "2            0          0          0           0           0         0  \n",
       "3            0          0          0           0           0         0  \n",
       "4            0          0          0           0           0         0  \n",
       "...        ...        ...        ...         ...         ...       ...  \n",
       "3011         0          0          0           0           0         0  \n",
       "3012         0          0          0           0           0         0  \n",
       "3013         0          0          0           0           0         0  \n",
       "3014         0          0          0           0           0         0  \n",
       "3015         0          0          0           0           0         0  \n",
       "\n",
       "[3016 rows x 9124 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram only\n",
    "cv_N_Grams = CountVectorizer(ngram_range=(2,2))\n",
    "X_N_Grams = cv_N_Grams.fit_transform(train['input'])\n",
    "print(X_N_Grams.shape)\n",
    "#document term matrix\n",
    "print(X_N_Grams.toarray())\n",
    "df = pd.DataFrame(X_N_Grams.toarray(),columns=cv_N_Grams.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "398c2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 20704)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability for</th>\n",
       "      <th>ability for students</th>\n",
       "      <th>ability to</th>\n",
       "      <th>ability to perform</th>\n",
       "      <th>ability to think</th>\n",
       "      <th>able to</th>\n",
       "      <th>able to buy</th>\n",
       "      <th>able to deal</th>\n",
       "      <th>able to do</th>\n",
       "      <th>able to end</th>\n",
       "      <th>...</th>\n",
       "      <th>your work</th>\n",
       "      <th>your work in</th>\n",
       "      <th>your work your</th>\n",
       "      <th>youth end</th>\n",
       "      <th>youth have</th>\n",
       "      <th>youth have less</th>\n",
       "      <th>youths who</th>\n",
       "      <th>youths who acquire</th>\n",
       "      <th>ypu fact</th>\n",
       "      <th>ypu fact end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3011</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3016 rows  20704 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ability for  ability for students  ability to  ability to perform  \\\n",
       "0               0                     0           0                   0   \n",
       "1               0                     0           0                   0   \n",
       "2               0                     0           0                   0   \n",
       "3               0                     0           0                   0   \n",
       "4               0                     0           0                   0   \n",
       "...           ...                   ...         ...                 ...   \n",
       "3011            0                     0           0                   0   \n",
       "3012            0                     0           0                   0   \n",
       "3013            0                     0           0                   0   \n",
       "3014            0                     0           0                   0   \n",
       "3015            0                     0           0                   0   \n",
       "\n",
       "      ability to think  able to  able to buy  able to deal  able to do  \\\n",
       "0                    0        0            0             0           0   \n",
       "1                    0        0            0             0           0   \n",
       "2                    0        0            0             0           0   \n",
       "3                    0        0            0             0           0   \n",
       "4                    0        0            0             0           0   \n",
       "...                ...      ...          ...           ...         ...   \n",
       "3011                 0        0            0             0           0   \n",
       "3012                 0        0            0             0           0   \n",
       "3013                 0        0            0             0           0   \n",
       "3014                 0        0            0             0           0   \n",
       "3015                 0        0            0             0           0   \n",
       "\n",
       "      able to end  ...  your work  your work in  your work your  youth end  \\\n",
       "0               0  ...          0             0               0          0   \n",
       "1               0  ...          0             0               0          0   \n",
       "2               0  ...          0             0               0          0   \n",
       "3               0  ...          0             0               0          0   \n",
       "4               0  ...          0             0               0          0   \n",
       "...           ...  ...        ...           ...             ...        ...   \n",
       "3011            0  ...          0             0               0          0   \n",
       "3012            0  ...          0             0               0          0   \n",
       "3013            0  ...          0             0               0          0   \n",
       "3014            0  ...          0             0               0          0   \n",
       "3015            0  ...          0             0               0          0   \n",
       "\n",
       "      youth have  youth have less  youths who  youths who acquire  ypu fact  \\\n",
       "0              0                0           0                   0         0   \n",
       "1              0                0           0                   0         0   \n",
       "2              0                0           0                   0         0   \n",
       "3              0                0           0                   0         0   \n",
       "4              0                0           0                   0         0   \n",
       "...          ...              ...         ...                 ...       ...   \n",
       "3011           0                0           0                   0         0   \n",
       "3012           0                0           0                   0         0   \n",
       "3013           0                0           0                   0         0   \n",
       "3014           0                0           0                   0         0   \n",
       "3015           0                0           0                   0         0   \n",
       "\n",
       "      ypu fact end  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "3011             0  \n",
       "3012             0  \n",
       "3013             0  \n",
       "3014             0  \n",
       "3015             0  \n",
       "\n",
       "[3016 rows x 20704 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean bigram and trigram\n",
    "cv_N_Grams = CountVectorizer(ngram_range=(2,3))\n",
    "X_N_Grams = cv_N_Grams.fit_transform(train['input'])\n",
    "print(X_N_Grams.shape)\n",
    "#document term matrix\n",
    "print(X_N_Grams.toarray())\n",
    "df = pd.DataFrame(X_N_Grams.toarray(),columns=cv_N_Grams.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564eb58",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "751d8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def clean_text(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    tokens = word_tokenize(txt_nopunct)\n",
    "    txt_clean = [word for word in tokens if word not in stopwords]\n",
    "    tokens_stem = [ps.stem(word) for word in txt_clean]\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f07f0be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 1877)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abou</th>\n",
       "      <th>absenc</th>\n",
       "      <th>academ</th>\n",
       "      <th>acadm</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accid</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>yellowston</th>\n",
       "      <th>yet</th>\n",
       "      <th>yong</th>\n",
       "      <th>york</th>\n",
       "      <th>youg</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youth</th>\n",
       "      <th>ypu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abil  abl  abou  absenc  academ  acadm  accept  access  accid  accord  ...  \\\n",
       "0   0.0  0.0   0.0     0.0     0.0    0.0     0.0     0.0    0.0     0.0  ...   \n",
       "1   0.0  0.0   0.0     0.0     0.0    0.0     0.0     0.0    0.0     0.0  ...   \n",
       "2   0.0  0.0   0.0     0.0     0.0    0.0     0.0     0.0    0.0     0.0  ...   \n",
       "3   0.0  0.0   0.0     0.0     0.0    0.0     0.0     0.0    0.0     0.0  ...   \n",
       "4   0.0  0.0   0.0     0.0     0.0    0.0     0.0     0.0    0.0     0.0  ...   \n",
       "\n",
       "   yellowston  yet  yong  york  youg  young  younger  youngster  youth  ypu  \n",
       "0         0.0  0.0   0.0   0.0   0.0    0.0      0.0        0.0    0.0  0.0  \n",
       "1         0.0  0.0   0.0   0.0   0.0    0.0      0.0        0.0    0.0  0.0  \n",
       "2         0.0  0.0   0.0   0.0   0.0    0.0      0.0        0.0    0.0  0.0  \n",
       "3         0.0  0.0   0.0   0.0   0.0    0.0      0.0        0.0    0.0  0.0  \n",
       "4         0.0  0.0   0.0   0.0   0.0    0.0      0.0        0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 1877 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec_train = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vec_train_fit = tfidf_vec_train.fit(train['input'])\n",
    "X_tfidf = tfidf_vec_train.fit_transform(train['input'])\n",
    "print(X_tfidf.shape)\n",
    "df = pd.DataFrame(X_tfidf.toarray(),columns=tfidf_vec_train.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fc120",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9f32f9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>input_POS</th>\n",
       "      <th>input_len</th>\n",
       "      <th>target_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt; start &gt; so i think we can not live if old pe...</td>\n",
       "      <td>So I think we would not be alive if our ancest...</td>\n",
       "      <td>JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...</td>\n",
       "      <td>125</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt; start &gt; so i think we can not live if old pe...</td>\n",
       "      <td>So I think we could not live if older people d...</td>\n",
       "      <td>JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...</td>\n",
       "      <td>125</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt; start &gt; so i think we can not live if old pe...</td>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "      <td>JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...</td>\n",
       "      <td>125</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt; start &gt; so i think we can not live if old pe...</td>\n",
       "      <td>So I think we can not live if old people can n...</td>\n",
       "      <td>JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...</td>\n",
       "      <td>125</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt; start &gt; for not use car &lt; end &gt;</td>\n",
       "      <td>Not for use with a car .</td>\n",
       "      <td>JJ NN NN IN RB JJ NN JJ NN NN</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  < start > so i think we can not live if old pe...   \n",
       "1  < start > so i think we can not live if old pe...   \n",
       "2  < start > so i think we can not live if old pe...   \n",
       "3  < start > so i think we can not live if old pe...   \n",
       "4                  < start > for not use car < end >   \n",
       "\n",
       "                                              target  \\\n",
       "0  So I think we would not be alive if our ancest...   \n",
       "1  So I think we could not live if older people d...   \n",
       "2  So I think we can not live if old people could...   \n",
       "3  So I think we can not live if old people can n...   \n",
       "4                          Not for use with a car .    \n",
       "\n",
       "                                           input_POS  input_len  target_len  \n",
       "0  JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...        125          94  \n",
       "1  JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...        125          88  \n",
       "2  JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...        125         108  \n",
       "3  JJ NN NNP RB JJ VBP PRP MD RB VB IN JJ NNS MD ...        125         111  \n",
       "4                      JJ NN NN IN RB JJ NN JJ NN NN         33          25  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# Function to calculate sentence length and complexity\n",
    "def calculate_sentence_features(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Calculate average sentence length\n",
    "    total_sentence_length = sum(len(sentence.split()) for sentence in sentences)\n",
    "    avg_sentence_length = total_sentence_length / len(sentences)\n",
    "    \n",
    "    # Calculate type-token ratio\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    types = len(set(words))\n",
    "    tokens = len(words)\n",
    "    ttr = types / tokens\n",
    "    \n",
    "    return avg_sentence_length, ttr\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "text = \"This is a sample sentence. It has multiple clauses and is of average length.\"\n",
    "avg_sentence_length, ttr = calculate_sentence_features(text)\n",
    "print(avg_sentence_length)\n",
    "print(ttr)\n",
    "\n",
    "train['input_len'] = train['input'].apply(lambda x: len(x))\n",
    "train.head()\n",
    "\n",
    "train['target_len'] = train['target'].apply(lambda x: len(x))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92633749",
   "metadata": {},
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12a0fabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think we can not live if old people could not find siences and tecnologies and they did not developped end\n",
      "so that example you go this should more agree are bring this almost transportation the concerned 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define a list of input text strings\n",
    "texts = train['input']\n",
    "# Create a tokenizer and fit on the text data\n",
    "vocab_size = 10000000\n",
    "oov_tok = '0' # Out of Vocabulary\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "word_index = tokenizer.word_index\n",
    "tokenizer.fit_on_texts(texts)\n",
    "# Convert the text data to a sequence of integer-encoded tokens\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# Pad the sequences to a maximum length of 20\n",
    "max_length = 20\n",
    "input_padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(tokenizer.sequences_to_texts(input_padded_sequences)[1])\n",
    "\n",
    "# Define a list of text strings\n",
    "texts2 = train['target']\n",
    "# Create a tokenizer and fit on the text data\n",
    "tokenizer2 = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "word_index2 = tokenizer2.word_index\n",
    "tokenizer2.fit_on_texts(texts2)\n",
    "# Convert the text data to a sequence of integer-encoded tokens\n",
    "sequences2 = tokenizer2.texts_to_sequences(texts2)\n",
    "target_padded_sequences = pad_sequences(sequences2, maxlen=max_length, padding='post')\n",
    "print(tokenizer.sequences_to_texts(target_padded_sequences)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14387a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edaf9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def decoder_input_data (text):\n",
    "    # Define a 2D numpy array\n",
    "    data = np.array(text)\n",
    "    # Shift the data to the right by one position and replace first column with zeros\n",
    "    shifted_data = np.roll(data, 1, axis=1)\n",
    "    shifted_data[:, 0] = 0\n",
    "    return shifted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "072bad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "latent_dim = 256\n",
    "batch_size = 32 \n",
    "epochs = 10 \n",
    "input_dim = 20\n",
    "output_dim = 20\n",
    "\n",
    "encoder_input_data = input_padded_sequences\n",
    "decoder_input_data  = decoder_input_data (input_padded_sequences)\n",
    "decoder_target_data = target_padded_sequences\n",
    "\n",
    "# Define the input sequence\n",
    "encoder_inputs = Input(shape=(None, input_dim))\n",
    "\n",
    "# Define the encoder LSTM\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# Get the encoder outputs and states\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "\n",
    "# Discard the encoder outputs and only keep the states\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder inputs\n",
    "decoder_inputs = Input(shape=(None, output_dim))\n",
    "\n",
    "# Define the decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get the decoder outputs and states\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Define the output layer\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "\n",
    "# Apply the output layer to the decoder outputs\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model inputs and outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99f48b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 20), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, 20).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 20) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 20), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, 20).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_3' (type Functional).\n    \n    Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 20)\n    \n    Call arguments received by layer 'model_3' (type Functional):\n       inputs=('tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)')\n       training=True\n       mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19748\\2029244217.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n\u001b[0m\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           validation_split=0.2)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\TREIKA\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 232, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model_3' (type Functional).\n    \n    Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 20)\n    \n    Call arguments received by layer 'model_3' (type Functional):\n       inputs=('tf.Tensor(shape=(None, 20), dtype=int32)', 'tf.Tensor(shape=(None, 20), dtype=int32)')\n       training=True\n       mask=None\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e972ff",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp, padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1,units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "    return result, sentence\n",
    "    \n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a470476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
